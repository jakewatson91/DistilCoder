training:
  max_length: 2048
  epochs: 3
  max_steps: -1
  batch_size: 1
  gradient_accumulation_steps: 8
  gradient_checkpointing: true
  learning_rate: 5e-5
  weight_decay: 1e-5
  warmup_ratio: 0.01
  seed: 42
  fp16: false
  bf16: true
  logging_steps: 50
  dataloader_num_workers: 2
  dataset_num_proc: 2

lora:
  r: 64
  alpha: 128
  dropout: 0.0
  bias: none
  task_type: CAUSAL_LM
